hydra:
  run:
    dir: outputs
network_dir: "."
initialization_network_dir: ''

cuda_graphs: false
jit: false

custom:
  gradient_method: fdm
  ntrain: 1000
  ntest: 100

arch:
  fno:
    dimension: 2
    nr_fno_layers: 4
    fno_layer_size: 32
    fno_modes: 12
    padding: 9
    output_fc_layer_sizes: 
      - 128

save_filetypes: np
summary_histograms: false
jit_use_nvfuser: false
find_unused_parameters: false
broadcast_buffers: false
device: ''
debug: false
run_mode: train

optimizer:
  _params_:
    compute_gradients: adam_compute_gradients
    apply_gradients: adam_apply_gradients
  _target_: torch.optim.Adam
  lr: 0.001
  betas:
  - 0.9
  - 0.999
  eps: 1.0e-08
  weight_decay: 0.0
  amsgrad: false
scheduler:
  _target_: custom
  _name_: tf.ExponentialLR
  decay_rate: 0.95
  decay_steps: 1000
loss:
  _target_: modulus.loss.aggregator.Sum
  weights:
    u: 1.0
    diffusion_u: 1.0

training:
  rec_results_freq : 1000
  max_steps : 10000
  grad_agg_freq: 1
  rec_validation_freq: ${training.rec_results_freq}
  rec_inference_freq: ${training.rec_results_freq}
  rec_monitor_freq: ${training.rec_results_freq}
  rec_constraint_freq: 2000
  save_network_freq: 1000
  print_stats_freq: 100
  summary_freq: 1000
  amp: false
  amp_dtype: float16
  ntk:
    use_ntk: false
    save_name: null
    run_freq: 1000

batch_size:
  grid: 16
  validation: 8
